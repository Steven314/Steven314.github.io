{
  "hash": "c2f8e134118f3ae251a9abf2e3837f2f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Different Methods for Extracting JSON with R\"\ncategories: [R, JSON]\ndraft: false\necho: true\ncode-link: true\nlightbox: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \nreference-location: margin\nauthor:\n  - name: Steven Carter\n    email: steven.carter4@utoledo.edu\n    affiliation:\n      - name: The University of Toledo\n        city: Toledo\n        state: OH\n        url: https://www.utoledo.edu/\ndate: 3/6/2025\ndate-modified: 3/6/2025\n---\n\n\n\nFor the past month or so I have been working with nested JSON data.\nI wanted to find the fastest strategy for data extraction.\nMy initial attempt was using the `{tidyverse}`.\nI was put off by the time this took to run.\nNext, I tried using `{data.table}` which I am fairly unfamiliar with.\n\nI was reading up on other methods and came across a [post](https://mtmorgan.github.io/software/update/2024/01/25/rjsoncons-ndjson-performance.html) from [Martin Morgan](https://mtmorgan.github.io/).\nHe is the author of `{rjsoncons}`.\nHis post covered the use of [DuckDB for reading JSON](https://duckdb.org/docs/stable/data/json/overview.html) which was quite performant.\n\nWhile sick today I went through the same operation with each approach.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(data.table)\nlibrary(jsonlite)\n```\n:::\n\n\n\nThe JSON file I'll be using is from a NCAA basketball game and is located [here](https://data.ncaa.com/casablanca/game/6351263/boxscore.json).\nI'm not sure how the performance of these different approaches will vary depending on the structure of the JSON file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!file.exists(\"boxscore.json\")) {\n    download.file(\n        \"https://data.ncaa.com/casablanca/game/6351263/boxscore.json\", \n        destfile = \"boxscore.json\"\n    )\n}\n```\n:::\n\n\n\n# `{tidyverse}` Approach\n\nThis is the easiest to understand method in my opinion.\nIt simply reads the JSON file using `{jsonlite}`, pulls out some metadata about the teams, pulls out some player statistics, and joins them together.\nIt returns a `{tibble}`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyverse_style <- function() {\n    j <- fromJSON(\"boxscore.json\")\n\n    stats <- j$teams |> \n        tibble() |> \n        select(teamId, playerStats) |> \n        unnest(playerStats) |> \n        mutate(teamId = as.character(teamId))\n  \n    full_join(\n        j$meta$teams, \n        stats, \n        by = join_by(id == teamId)\n    )\n}\n```\n:::\n\n\n\n# `{data.table}` Approach\n\nThe process here is essentially the same as above.\nThis however returns a `data.frame`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.table_style <- function() {\n    j <- fromJSON(\"boxscore.json\")\n\n    teams <- data.table(j$teams)[, .(\n        teamId = as.character(teamId),\n        rbindlist(playerStats)\n    )]\n  \n    return(\n        merge(\n            j$meta$teams,\n            teams, \n            by.x = \"id\",\n            by.y = \"teamId\"\n        ) |> \n            setnames(\"id\", \"teamId\")\n    )\n}\n```\n:::\n\n\n\n\n# DuckDB Approach\n\nFor DuckDB we need to load up an in-memory database.\nAlso, we have to install and load the JSON extension for DuckDB.\n\nThis also returns a `data.frame`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(duckdb())\n\ndbExecute(con, \"INSTALL json\")\ndbExecute(con, \"LOAD json\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduck_style <- function() {\n    dbGetQuery(\n        con, \n        \"\n        with raw as (select meta, teams from 'boxscore.json'),\n        meta as (select unnest(meta.teams, recursive := true) from raw),\n        teams_large as (select unnest(teams) as team from raw),\n        teams as (\n            select\n                team.teamId::VARCHAR as teamId,\n                unnest(team.playerStats, recursive := true)\n            from teams_large\n        )\n        select * exclude meta.id\n        from meta, teams\n        where meta.id = teams.teamId\n        \"\n    )\n}\n```\n:::\n\n\n\n# Benchmark\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- microbenchmark::microbenchmark(\n  duck  = duck_style(),\n  tidy  = tidyverse_style(),\n  dt    = data.table_style(),\n  times = 100\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/fig-results-1.png){#fig-results width=480}\n:::\n:::\n\n\n\n::: {.callout-note}\n\nIn my testing with a standard R script, DuckDB came out about 7ms faster than the tidy approach.\nI'm not sure what the difference is.\n\n:::\n\nWe see that `{data.table}` is the clear winner here with times below 10ms.\nDuckDB looks to be the most consistent.\n\nThere are probably some more optimizations that I am not seeing, but this is what I can up with in a few hours.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}