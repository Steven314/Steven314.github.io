{
  "hash": "db0238f753e0cdcd696371381aef8b8c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Balancing Classification Errors for Aggregate Consistency\"\ncategories: [Classification, R, Logistic Regression]\ndraft: false\necho: true\ncode-link: true\nlightbox: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \nreference-location: margin\nauthor:\n  - name: Steven Carter\n    email: steven.carter4@utoledo.edu\n    affiliation:\n      - name: The University of Toledo\n        city: Toledo\n        state: OH\n        url: https://www.utoledo.edu/\ndate: 11/29/2024\ndate-modified: 11/29/2024\n---\n\n\n\n# Introduction\n\nRecently, I was developing a model to predict a binary outcome.\nI settled on a logistic regression model since that was something I was somewhat familiar with from my Econometrics class.\nFrom a logistic regression model you estimate the probability of the observation belonging to the positive class.\nAs one would guess, a probability greater than 50% would indicate the positive class, and below 50%, the negative class.\nThis is the threshold value; the threshold where the classification changes from one outcome to the other.\n\n$$\n\\hat{y_i} = \\begin{cases} \np(x_i) \\geq t, &\\text{then } 1 \\\\\np(x_i) < t, &\\text{then } 0\n\\end{cases}\n$$ {#eq-threshold}\nwhere $t$ is the threshold value and $t\\in[0,1]$.\n\n# Problem\n\nHowever, the response variable was quite imbalanced; about 75% of one class and 25% of the other.^[The example data I'll be using is slightly imbalanced, 51.6%.]\nIn checking the fit I noticed the model was overestimating the majority class.\nThis was problematic as the objective was to build a model that would form an accurate estimate of the total number (or proportion) of positive cases while looking at each individual in the sample.\nThe individuals estimated to be in the positive class would then be used for further estimation, but that is not the point of this exercise.\n\nWhen I had come across this situation in the past I tried varying the threshold value manually to manipulate the quantity of positive classifications.\nThis worked okay, but was cumbersome and annoying.\nI thought that there must be a better way to determine the threshold.\n\nMy second attempt to solve this issue came in the form of weights\nBy manually adjusting the weight for misclassification along with the resulting confusion matrix, I would attempt to balance the quantities of errors so that the overall prediction prevalence of the positive class was similar to the training data.\nThis again worked okay for my purposes, but I knew there still had to be a better way.\n\n# Solution\n\nI messed around with adjusting a few methods for handling imbalanced classes, but none of them seemed to help.\n\nThe objective was to neither over- or underestimate the number of positive cases.\nAnother way of saying that is that I need to have the same number of Type I (false positive) and Type II (false negative) errors.\n\nLooking at a confusion matrix (@fig-conf-mat), taking Class 1 as our positive class, we have the number of Type I errors in the top-right and the number of Type II errors in the bottom-left.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(yardstick) # two_class_example is from yardstick\nlibrary(ggplot2)\n\ntwo_class_example |> \n  conf_mat(truth, predicted) |> \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"lightblue\") +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![A confusion matrix displaying the results from the `two_class_example` dataset.\n](index_files/figure-html/fig-conf-mat-1.png){#fig-conf-mat width=480}\n:::\n:::\n\n\n\nWhile reading through the documentation for [`{yardstick}`](https://yardstick.tidymodels.org/), I came across Precision-Recall (PR) Curves ([`pr_curve`](https://yardstick.tidymodels.org/reference/pr_curve.html)).\nWhat is convenient about this curve is that it checks the precision and recall for each unique probability found in your dataset.\nThis ends up being very useful for us.\nTo see how we have to look at the definitions of precision and recall.\n\n## Precision and Recall\n\nThere are many different websites that provide a definition for these metrics.\nYou can easily find several detailed explanations with a quick search.\n\n$$\n\\begin{split}\n\\text{Precision}&= \\frac{\\text{True Positive}}{\\text{True Positive}+ \\text{False Positive}} = \\frac{TP}{TP+FP} \\\\\n\\text{Recall} &= \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{TP}{TP+FN}\n\\end{split}\n$$ {#eq-precision-recall}\n\nIf we rearrange these definitions to solve for $FP$ and $FN$, we can make some progress.\n\n$$\n\\begin{aligned}\n\\text{Precision}\\cdot(TP+FP) &= TP \\\\\n(TP + FP) &= \\frac{TP}{\\text{Precision}} \\\\\nFP &= \\frac{TP}{\\text{Precision}} - TP \\\\\nFP &= TP \\cdot \\left(\\frac{1}{\\text{Precision}}\\right)\n\\end{aligned}\n$$ {#eq-precision}\n\nThe same algebraic process can be done for recall which gives,\n\n$$\nFN = TP \\cdot \\left(\\frac{1}{\\text{Recall}}\\right)\n$$ {#eq-recall}\n\nNow looking back to our objective, to make the quantities of Type I and Type II errors equal, we should set $FP$ and $FN$ equal to each other.\n\n$$\n\\begin{aligned}\nFN &= FP \\\\\nTP \\cdot \\left(\\frac{1}{\\text{Recall}}\\right) &= TP \\cdot \\left(\\frac{1}{\\text{Precision}}\\right) \\\\\n\\frac{1}{\\text{Recall}} &= \\frac{1}{\\text{Precision}} \\\\\n \\text{Precision} &= \\text{Recall}\n\\end{aligned}\n$$ {#eq-recall-precision-equal}\n\nSo we see that our error types will be equal in quantity when our precision and recall are equal.\n\n## PR Curve\n\nNow that we know what we want to achieve, we have to write the code to do it.\n\nUsing [`pr_curve`](https://yardstick.tidymodels.org/reference/pr_curve.html), we will shortcut a search of the possible threshold values ranging from zero to one.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\npr <- two_class_example |> \n  as_tibble() |> \n  pr_curve(truth, Class1)\n\nhead(pr)\n#> # A tibble: 6 × 3\n#>   .threshold  recall precision\n#>        <dbl>   <dbl>     <dbl>\n#> 1     Inf    0               1\n#> 2       1.00 0.00388         1\n#> 3       1.00 0.00775         1\n#> 4       1.00 0.0116          1\n#> 5       1.00 0.0155          1\n#> 6       1.00 0.0194          1\n```\n:::\n\n\n\nWe can plot the curve to find the point where precision and recall are equal.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(pr) +\n  geom_abline(slope = 1, intercept = 0, color = \"grey\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![A PR curve for the `two_class_example` dataset.\n](index_files/figure-html/fig-pr-curve-1.png){#fig-pr-curve width=480}\n:::\n:::\n\n\n\nWe see that precision and recall are equal along the dashed line and this meets the PR curve at about (0.85, 0.85).\nThe exact point would be available in `pr`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npr_diff <- pr |> \n  mutate(diff = abs(precision - recall)) |> \n  arrange(diff)\n\nhead(pr_diff)\n#> # A tibble: 6 × 4\n#>   .threshold recall precision    diff\n#>        <dbl>  <dbl>     <dbl>   <dbl>\n#> 1      0.610  0.864     0.864 0      \n#> 2      0.618  0.860     0.864 0.00335\n#> 3      0.602  0.868     0.865 0.00335\n#> 4      0.601  0.868     0.862 0.00668\n#> 5      0.618  0.857     0.863 0.00669\n#> 6      0.590  0.868     0.858 0.00998\n\nthreshold <- pr_diff |> \n  slice(1) |> \n  pull(.threshold)\n```\n:::\n\n\n\nThis tells us that a threshold of 0.61 will provide us with a balanced number of positive cases.\nWe can check this my looking at the confusion matrix when we use 0.61 as the threshold.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwo_class_adjusted <- two_class_example |> \n  mutate(\n    predicted_threshold = factor(\n      if_else(Class1 >= threshold, \"Class1\", \"Class2\"),\n      levels = levels(two_class_example$truth)\n    )\n  )\n\ntwo_class_adjusted |> \n  conf_mat(truth, predicted_threshold) |> \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"lightblue\") +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![The confusion matrix with the adjusted threshold value.\n](index_files/figure-html/fig-adjusted-conf-mat-1.png){#fig-adjusted-conf-mat width=480}\n:::\n:::\n\n\n\nNotice how now the lower-left and upper-right quadrants have equal values.\nThis means that the number of observations classified as Class 1 matches the amount in the training data.\n\nHowever, this can come at the cost of reduced accuracy.\nFunnily enough this is not the case for this particular dataset, but it was for the dataset I was using.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\n\ntwo_class_adjusted |> \n  select(truth, predicted, predicted_threshold) |> \n  pivot_longer(\n    cols            = c(predicted, predicted_threshold),\n    names_to        = \"threshold\",\n    values_to       = \"predicted\",\n    names_transform = \\(x) ifelse(x == \"predicted\", 0.5, threshold)\n  ) |> \n  group_by(threshold) |> \n  accuracy(truth, predicted) |> \n  select(threshold, accuracy = .estimate)\n#> # A tibble: 2 × 2\n#>   threshold accuracy\n#>       <dbl>    <dbl>\n#> 1     0.5      0.838\n#> 2     0.610    0.86\n```\n:::\n\n\n\nOf course, in reality you would want to find the best threshold for each training set and average them to apply to the test set.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}