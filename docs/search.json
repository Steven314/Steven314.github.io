[
  {
    "objectID": "tech.html",
    "href": "tech.html",
    "title": "Technologies I Use",
    "section": "",
    "text": "I have a Raspberry Pi 5 which runs a handful of Docker containers. I don’t make use of them very often, but it’s a nice hobby.\n\nCasaOS for general management.\nJellyfin for music.\nGitea for local version control.\nHomepage for a custom browser homepage.\nMemos for the local sharing of memes.\n\nUsing v0.24.3 since I really don’t like the UI changes in more recent versions.\n\nOpen WebUI to experiment with self-hosted AI models through Ollama.\n\nA Raspberry Pi 5 (8GB) is not capable of very much when it comes to AI (really only &lt;3b parameters given the other things running).\n\nCrafty for a highly neglected and currently broken Minecraft server.\n\nThese are all wired up to subdomains for easy (local-only) access."
  },
  {
    "objectID": "tech.html#languages",
    "href": "tech.html#languages",
    "title": "Technologies I Use",
    "section": "Languages",
    "text": "Languages\n\nProgramming\n\n\n\nLanguage\nProficiency\nExperience\n\n\n\n\nR\nStrong\n2.5 years\n\n\nPython\nModerate\n2 years\n\n\nJava\nBasic\n2 semesters\n\n\nCSS\nBasic\nSome\n\n\nJavaScript\nNear Zero\nNext to none\n\n\n\n\n\nQuery\n\n\n\nLanguage\nProficiency\nExperience\n\n\n\n\nSQL\nModerate\n1.5 years\n\n\nDAX\nBasic\nSome\n\n\n\n\n\nMarkup\n\n\n\nLanguage\nProficiency\nExperience\n\n\n\n\nMarkdown\nStrong\n2.5 years\n\n\nQuarto\nStrong\n2.5 years\n\n\nLaTeX\nModerate\n1 year\n\n\nHTML\nBasic\nSome\n\n\nTypst\nBasic\nSome"
  },
  {
    "objectID": "tech.html#software",
    "href": "tech.html#software",
    "title": "Technologies I Use",
    "section": "Software",
    "text": "Software\nThis is the software I regularly use or have tried.\n\nRStudio: This is my go-to editor for writing reports and apps with Quarto and Shiny. The open-source community is really thriving and makes the R language very approachable. A big plus is the Vim motions as I use a weird combination of Vim motions and keyboard macros.\nVS Code: I use this for miscellaneous code editing and any Python work. It gets the job done.\n\nPositron: It’s a weird cross between RStudio and VS Code.\nWindsurf: Basically VS Code with AI. The AI portion is kind of in the way. It is useful for file paths and tedious repetitive edits, but I don’t find it very useful for real work.\n\nPower BI: I am not a fan of any Microsoft data analysis software. It feels restrictive to do anything outside of basic charts and graphs. I always feel that I am fighting the program to get what I want. Being locked down in a closed-source, paid ecosystem is a big negative. The documentation is severely lacking compared to that of the R community.\n\nDAX: This is terrible. Just terrible.\nSQL Server Analysis Server: It uses DAX. It’s okay.\n\nTableau: Similar thoughts to Power BI, but I have generally had a better experience. However, I have used it very little.\nArcGIS: I only used this for a class. It was an alright experience, but I haven’t done any complex work to feel out the program yet. All of my mapping needs recently have been filled by {plotly} and {ggplot2}.\nObsidian: This is a good and adaptable notetaking software. I use it often on all my devices.\nDuckDB: This is a good database software. I use it mainly for ETL into local data marts which are then used in Shiny applications or reporting with Quarto. For me this is superior to having a server-based warehouse as the data are fairly small in size and can be programmatically refreshed with an R script. The SQL dialect is very good in my opinion. It has very useful shorthands compared to standard SQL."
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html",
    "href": "posts/2025-10-06-distance-lab/index.html",
    "title": "Distance Calculations Lab Using R",
    "section": "",
    "text": "This lab walks through an example analysis using Lucas County housing data from the 1990s. It was presented on October 6, 2025 to the SISS7020 PhD class at the University of Toledo."
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html#exploratory-data-analysis",
    "href": "posts/2025-10-06-distance-lab/index.html#exploratory-data-analysis",
    "title": "Distance Calculations Lab Using R",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\nCode\nggplot(\n  data = house_sf_dist,\n  aes(\n    x = distance_haversine, \n    y = price\n  )\n) +\n  geom_point(alpha = 0.1) +\n  scale_y_log10(\n    labels = scales::label_currency(\n      scale_cut = scales::cut_short_scale()\n    )\n  ) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(\n    x = \"Haversine Distance (km)\",\n    y = \"Sale Price\",\n    title = \"Housing Sales in Lucas County, 1993-1998\"\n  )\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWith log-scales on both the \\(x\\) and \\(y\\) axes there seems to be a mostly linear relationship. That is convenient.\n\n\nCode\nggplot(\n  data = house_sf_dist,\n  aes(\n    x = distance_haversine, \n    y = price\n  )\n) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  scale_y_log10(\n    labels = scales::label_currency(\n      scale_cut = scales::cut_short_scale()\n    )\n  ) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(\n    x = \"Haversine Distance (km)\",\n    y = \"Sale Price\",\n    title = \"Housing Sales in Lucas County, 1993-1998\"\n  )\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nWhat about the other variables? How might housing prices vary for reasons other than distance? Or, how might distance to correlated with other variables?\nHere are the variables we have:\n\nglimpse(house_sf_dist)\n#&gt; Rows: 25,357\n#&gt; Columns: 32\n#&gt; $ price              &lt;int&gt; 303000, 92000, 90000, 330000, 185000, 100000, 43065…\n#&gt; $ yrbuilt            &lt;int&gt; 1978, 1957, 1937, 1887, 1978, 1989, 1927, 1990, 187…\n#&gt; $ stories            &lt;fct&gt; one+half, one, two, one+half, two, one, one, one+ha…\n#&gt; $ TLA                &lt;int&gt; 3273, 920, 1956, 1430, 2208, 1232, 832, 3011, 1034,…\n#&gt; $ wall               &lt;fct&gt; partbrk, metlvnyl, stucdrvt, wood, partbrk, wood, w…\n#&gt; $ beds               &lt;dbl&gt; 4, 2, 3, 4, 3, 1, 2, 3, 3, 2, 4, 5, 4, 3, 3, 3, 3, …\n#&gt; $ baths              &lt;dbl&gt; 3, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, …\n#&gt; $ halfbaths          &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, …\n#&gt; $ frontage           &lt;int&gt; 177, 100, 192, 200, 241, 154, 320, 362, 500, 220, 2…\n#&gt; $ depth              &lt;int&gt; 0, 0, 0, 217, 0, 1420, 0, 0, 990, 0, 0, 0, 0, 0, 0,…\n#&gt; $ garage             &lt;fct&gt; attached, detached, attached, no garage, attached, …\n#&gt; $ garagesqft         &lt;int&gt; 625, 308, 480, 0, 528, 1200, 360, 672, 624, 780, 61…\n#&gt; $ rooms              &lt;int&gt; 12, 4, 7, 7, 7, 5, 4, 6, 7, 6, 6, 10, 8, 5, 6, 6, 6…\n#&gt; $ lotsize            &lt;int&gt; 53496, 37900, 52900, 43560, 225800, 214100, 23500, …\n#&gt; $ sdate              &lt;int&gt; 960423, 970421, 931101, 971223, 950807, 931101, 951…\n#&gt; $ avalue             &lt;int&gt; 306514, 84628, 126514, 199228, 192514, 107914, 4902…\n#&gt; $ s1993              &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n#&gt; $ s1994              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ s1995              &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ s1996              &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ s1997              &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, …\n#&gt; $ s1998              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, …\n#&gt; $ syear              &lt;fct&gt; 1996, 1997, 1993, 1997, 1995, 1993, 1995, 1993, 199…\n#&gt; $ age                &lt;dbl&gt; 18, 40, 56, 110, 17, 4, 68, 3, 119, 60, 41, 29, 0, …\n#&gt; $ geometry           &lt;POINT [°]&gt; POINT (-83.87964 41.4169), POINT (-83.87716 4…\n#&gt; $ longitude          &lt;dbl&gt; -83.87964, -83.87716, -83.87271, -83.86670, -83.838…\n#&gt; $ latitude           &lt;dbl&gt; 41.41690, 41.41720, 41.41773, 41.42522, 41.42928, 4…\n#&gt; $ toledo_lat         &lt;dbl&gt; 41.65278, 41.65278, 41.65278, 41.65278, 41.65278, 4…\n#&gt; $ toledo_lng         &lt;dbl&gt; -83.53778, -83.53778, -83.53778, -83.53778, -83.537…\n#&gt; $ distance_euclidean &lt;dbl&gt; 46.18378, 45.93799, 45.49884, 44.47400, 41.63927, 4…\n#&gt; $ distance_manhattan &lt;dbl&gt; 64.24187, 63.93241, 63.37922, 61.87764, 58.26169, 6…\n#&gt; $ distance_haversine &lt;dbl&gt; 38.74246, 38.56770, 38.25653, 37.32047, 35.29493, 3…\n\nInformation about each variable can be found in the associated help file. It can be opened by typing ?house in the console. Although it is not the most thorough explanations of each variable, it is still helpful.\nWe can make a list of relationships to consider. In these there are plenty of hypotheses.\n\nAs you move toward the city center, lots get smaller, thus prices go down. The converse also being the case.\nHaving more bedrooms, bathrooms, or stories would increase price.\n\nThis may be correlated with lot size.\n\nThe city center may be older and perhaps older houses have lower prices.\n\nAll of those are statements we can test.\nWe can also come up with statements we can’t test with the data currently at hand.\n\nHouses close to highways have lower value.\nProximity to amenities, such as schools, parks, and entertainment, can contribute to changes in housing prices.\nA house of poor quality would have lower value.\n\n\nSpatial Variation\n\n\n\n\n\n\nNote\n\n\n\nSpatial variance is something I would expect us to cover in an upcoming spatial statistics class. For now, I’ll cover how some variables are correlated with distance from the city center and price.\n\n\n\nAge\n\n\nCode\nggplot(\n  data = house_sf_dist,\n  aes(\n    x = yrbuilt,\n    y = price\n  )\n) + \n  geom_point(alpha = 0.1) +\n  scale_y_log10(\n    labels = scales::label_currency(\n      scale_cut = scales::cut_short_scale()\n    )\n  ) +\n  theme_bw() +\n  labs(\n    x = \"Year Build\",\n    y = \"Sale Price\"\n  )\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nLook at the final years. Something weird seems to be happening with houses built and sold at the end of the dataset. There houses would have a very young age. (Some houses even have a negative age.) Perhaps this means the sale price is reflecting the price of an empty lot and not the price of a house. This could be a reason to exclude houses with an age less than some threshold, say 2 years.\n\n\nCode\nggplot(\n  data = house_sf_dist,\n  aes(\n    x = distance_haversine,\n    y = yrbuilt\n  )\n) + \n  geom_point(alpha = 0.1) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(\n    x = \"Distance to City Center (km)\",\n    y = \"Year Built\"\n  )\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat might have caused those two bands where there are no houses being build (or at least sold)?"
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html#linear-model",
    "href": "posts/2025-10-06-distance-lab/index.html#linear-model",
    "title": "Distance Calculations Lab Using R",
    "section": "Linear Model",
    "text": "Linear Model\nThe lm() function can be used for a linear model. It takes a dataset for training and a formula.\n\n\n\n\n\n\nSplitting Data\n\n\n\nWhen training a model to be used for predicting new data you should split into training and testing (and potentially validation) sets. When there is a spatial component you may also want to consider the spatial relationships.\n\nSpatial Data Science seems like a good resource for more info on this. I have yet to read it, but it is on the list for the future.\nTidy Modeling with R, chapter 5 is also a good resource for information on training a model. I have read this whole book and can recommend it.\n{spatialsample} is a package in the tidymodels and tidyverse ecosystem that can perform spatial sampling. I haven’t used it yet, but likely will within the next year.\n\n\n\nFirst let’s try a simple model with just distance and taking logs of distance and price.\n\nsimple_model &lt;- lm(\n  data = house_sf_dist,\n  formula = log(price) ~ log(distance_haversine)\n)\n\nsimple_model |&gt; \n  summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(price) ~ log(distance_haversine), data = house_sf_dist)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.6031 -0.2995  0.0568  0.3500  2.4195 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)             9.248718   0.012482   740.9   &lt;2e-16 ***\n#&gt; log(distance_haversine) 0.881602   0.005961   147.9   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.559 on 25355 degrees of freedom\n#&gt; Multiple R-squared:  0.4631, Adjusted R-squared:  0.4631 \n#&gt; F-statistic: 2.187e+04 on 1 and 25355 DF,  p-value: &lt; 2.2e-16\n\nThis has a decent \\(R^2\\) value. However, we can’t trust the model coefficients. Using only distance isn’t a very good model.\nWhy?\nOther factors have spatial variance. Distance isn’t the only thing changing between observations. We have an endogeneity issue or omitted variable bias. That should sound familiar from an econometrics class. If not, you should be covering that soon.\n\nbig_model &lt;- lm(\n  data = house_sf_dist,\n  formula = log(price) ~ log(distance_haversine) +\n    lotsize +\n    beds +\n    baths +\n    halfbaths +\n    stories +\n    garage +\n    TLA +\n    age +\n    syear\n)\n\nbig_model |&gt; \n  summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(price) ~ log(distance_haversine) + lotsize + \n#&gt;     beds + baths + halfbaths + stories + garage + TLA + age + \n#&gt;     syear, data = house_sf_dist)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -2.88571 -0.15787  0.05206  0.23625  2.58261 \n#&gt; \n#&gt; Coefficients:\n#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              9.481e+00  2.368e-02 400.384  &lt; 2e-16 ***\n#&gt; log(distance_haversine)  4.309e-01  6.338e-03  67.986  &lt; 2e-16 ***\n#&gt; lotsize                 -4.381e-07  9.750e-08  -4.493 7.05e-06 ***\n#&gt; beds                     1.989e-02  4.745e-03   4.191 2.79e-05 ***\n#&gt; baths                   -3.952e-02  8.022e-03  -4.927 8.42e-07 ***\n#&gt; halfbaths                7.278e-02  6.622e-03  10.991  &lt; 2e-16 ***\n#&gt; storiesbilevel          -1.842e-01  1.873e-02  -9.832  &lt; 2e-16 ***\n#&gt; storiesmultilvl         -7.237e-02  1.579e-02  -4.582 4.63e-06 ***\n#&gt; storiesone+half         -3.144e-02  8.927e-03  -3.522 0.000429 ***\n#&gt; storiestwo               9.575e-02  6.928e-03  13.820  &lt; 2e-16 ***\n#&gt; storiestwo+half          1.671e-01  2.857e-01   0.585 0.558637    \n#&gt; storiesthree            -6.321e-02  2.861e-01  -0.221 0.825137    \n#&gt; garagebasement           5.417e-01  4.653e-02  11.641  &lt; 2e-16 ***\n#&gt; garageattached           4.064e-01  9.997e-03  40.648  &lt; 2e-16 ***\n#&gt; garagedetached           4.054e-01  7.841e-03  51.703  &lt; 2e-16 ***\n#&gt; garagecarport            3.094e-01  2.837e-02  10.907  &lt; 2e-16 ***\n#&gt; TLA                      3.803e-04  7.851e-06  48.443  &lt; 2e-16 ***\n#&gt; age                     -7.909e-03  1.496e-04 -52.866  &lt; 2e-16 ***\n#&gt; syear1994                5.699e-02  9.693e-03   5.879 4.18e-09 ***\n#&gt; syear1995                9.373e-02  9.468e-03   9.899  &lt; 2e-16 ***\n#&gt; syear1996                1.061e-01  9.162e-03  11.581  &lt; 2e-16 ***\n#&gt; syear1997                1.566e-01  9.099e-03  17.213  &lt; 2e-16 ***\n#&gt; syear1998                2.397e-01  9.370e-03  25.582  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4039 on 25334 degrees of freedom\n#&gt; Multiple R-squared:   0.72,  Adjusted R-squared:  0.7198 \n#&gt; F-statistic:  2961 on 22 and 25334 DF,  p-value: &lt; 2.2e-16\n\nThis model improves the \\(R^2\\) value. Compare the coefficiencts for \\(\\log(d_\\text{haversine})\\). The coefficient in the second model is only half of that in the first model.\nThis means that some of the variation in price is better explained by the other variables we included."
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html#model-conclusion",
    "href": "posts/2025-10-06-distance-lab/index.html#model-conclusion",
    "title": "Distance Calculations Lab Using R",
    "section": "Model Conclusion",
    "text": "Model Conclusion\nIf we are happy with the reasoning for our model (i.e. we don’t expect any omitted variable bias or other statistical issues), we could attempt to claim a causal effect of distance on housing prices.\nThe predictions can be found with the predict() function.\nNotice how we have to use exp() wrapping predict. This is because we used log(price). We need to reverse the logarithm.\n\nhouse_sf_dist |&gt; \n  as_tibble() |&gt; \n  mutate(\n    price_hat = exp(predict(\n      big_model, \n      data = house_sf_dist\n    ))\n  ) |&gt; \n  select(price, price_hat)\n#&gt; # A tibble: 25,357 × 2\n#&gt;     price price_hat\n#&gt;     &lt;int&gt;     &lt;dbl&gt;\n#&gt;  1 303000   311781.\n#&gt;  2  92000   112826.\n#&gt;  3  90000   140216.\n#&gt;  4 330000    52063.\n#&gt;  5 185000   213576.\n#&gt;  6 100000   128987.\n#&gt;  7  43065    79742.\n#&gt;  8 305000   246901.\n#&gt;  9  56493    52171.\n#&gt; 10  50000    89339.\n#&gt; # ℹ 25,347 more rows"
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html#more-complicated-models",
    "href": "posts/2025-10-06-distance-lab/index.html#more-complicated-models",
    "title": "Distance Calculations Lab Using R",
    "section": "More Complicated Models",
    "text": "More Complicated Models\nThe actual model technique may not necessarily be more complicated, but the feature engineering (TMwR chapter 8) is fancier.\n\nIf a house in the same neighborhood (or at least nearby) sold the previous year, the housing prices in the neighborhood may be influenced. The proximity with a recently sold house and how similar (also distance, but in non-spatial dimensions) could affect how strong that influence is.\n\nI couldn’t find a paper that covered this, but I didn’t look for very long.\n\nProximity to other points-of-interest, such as schools, parks, entertainment, or highway access, can be considered.\n\nMeasuring Highway Impacts on House Prices Using Spatial Regression\n\nUsing travel time instead of straight line distance.\n\nRoad distance and travel time for an improved house price Kriging predictor"
  },
  {
    "objectID": "posts/2025-10-06-distance-lab/index.html#footnotes",
    "href": "posts/2025-10-06-distance-lab/index.html#footnotes",
    "title": "Distance Calculations Lab Using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLeaflet and Plotly each have R and Python interfaces to their JavaScript libraries. If you see a interactive map on a website, there is a good chance it was made with one of those libraries.↩︎"
  },
  {
    "objectID": "posts/2025-03-06-json-extraction/index.html",
    "href": "posts/2025-03-06-json-extraction/index.html",
    "title": "Different Methods for Extracting JSON with R",
    "section": "",
    "text": "For the past month or so I have been working with nested JSON data. I wanted to find the fastest strategy for data extraction. My initial attempt was using the tidyverse. I was put off by the time this took to run. Next, I tried using data.table which I am fairly unfamiliar with.\nI was reading up on other methods and came across a post from Martin Morgan. He is the author of rjsoncons. His post covered the use of DuckDB for reading JSON which was quite performant.\nWhile sick today I went through the same operation with each approach.\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(data.table)\nlibrary(jsonlite)\n\nThe JSON file I’ll be using is from a NCAA basketball game and is located here. I’m not sure how the performance of these different approaches will vary depending on the structure of the JSON file.\n\nif (!file.exists(\"boxscore.json\")) {\n    download.file(\n        \"https://data.ncaa.com/casablanca/game/6351263/boxscore.json\", \n        destfile = \"boxscore.json\"\n    )\n}\n\n\n{tidyverse} Approach\nThis is the easiest to understand method in my opinion. It simply reads the JSON file using jsonlite, pulls out some metadata about the teams, pulls out some player statistics, and joins them together. It returns a tibble.\n\ntidyverse_style &lt;- function() {\n    j &lt;- fromJSON(\"boxscore.json\")\n\n    stats &lt;- j$teams |&gt; \n        tibble() |&gt; \n        select(teamId, playerStats) |&gt; \n        unnest(playerStats) |&gt; \n        mutate(teamId = as.character(teamId))\n  \n    full_join(\n        j$meta$teams, \n        stats, \n        by = join_by(id == teamId)\n    )\n}\n\n\n{data.table} Approach\nThe process here is essentially the same as above. This however returns a data.frame.\n\ndata.table_style &lt;- function() {\n    j &lt;- fromJSON(\"boxscore.json\")\n\n    teams &lt;- data.table(j$teams)[, .(\n        teamId = as.character(teamId),\n        rbindlist(playerStats)\n    )]\n  \n    return(\n        merge(\n            j$meta$teams,\n            teams, \n            by.x = \"id\",\n            by.y = \"teamId\"\n        ) |&gt; \n            setnames(\"id\", \"teamId\")\n    )\n}\n\nDuckDB Approach\nFor DuckDB we need to load up an in-memory database. Also, we have to install and load the JSON extension for DuckDB.\nThis also returns a data.frame.\n\ncon &lt;- dbConnect(duckdb())\n\ndbExecute(con, \"INSTALL json\")\ndbExecute(con, \"LOAD json\")\n\n\nduck_style &lt;- function() {\n    dbGetQuery(\n        con, \n        \"\n        with raw as (select meta, teams from 'boxscore.json'),\n        meta as (select unnest(meta.teams, recursive := true) from raw),\n        teams_large as (select unnest(teams) as team from raw),\n        teams as (\n            select\n                team.teamId::VARCHAR as teamId,\n                unnest(team.playerStats, recursive := true)\n            from teams_large\n        )\n        select * exclude meta.id\n        from meta, teams\n        where meta.id = teams.teamId\n        \"\n    )\n}\n\nBenchmark\n\nresults &lt;- microbenchmark::microbenchmark(\n  duck  = duck_style(),\n  tidy  = tidyverse_style(),\n  dt    = data.table_style(),\n  times = 100\n)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn my testing with a standard R script, DuckDB came out about 7ms faster than the tidy approach. I’m not sure what the difference is.\n\n\nWe see that data.table is the clear winner here with times below 10ms. DuckDB looks to be the most consistent.\nThere are probably some more optimizations that I am not seeing, but this is what I can up with in a few hours.\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html",
    "href": "posts/2024-11-28-classification-errors/index.html",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "",
    "text": "Recently, I was developing a model to predict a binary outcome. I settled on a logistic regression model since that was something I was somewhat familiar with from my Econometrics class. From a logistic regression model you estimate the probability of the observation belonging to the positive class. As one would guess, a probability greater than 50% would indicate the positive class, and below 50%, the negative class. This is the threshold value; the threshold where the classification changes from one outcome to the other.\n\\[\n\\hat{y_i} = \\begin{cases}\np(x_i) \\geq t, &\\text{then } 1 \\\\\np(x_i) &lt; t, &\\text{then } 0\n\\end{cases}\n\\tag{1}\\] where \\(t\\) is the threshold value and \\(t\\in[0,1]\\)."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "href": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nThere are many different websites that provide a definition for these metrics. You can easily find several detailed explanations with a quick search.\n\\[\n\\begin{split}\n\\text{Precision}&= \\frac{\\text{True Positive}}{\\text{True Positive}+ \\text{False Positive}} = \\frac{TP}{TP+FP} \\\\\n\\text{Recall} &= \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{TP}{TP+FN}\n\\end{split}\n\\tag{2}\\]\nIf we rearrange these definitions to solve for \\(FP\\) and \\(FN\\), we can make some progress.\n\\[\n\\begin{aligned}\n\\text{Precision}\\cdot(TP+FP) &= TP \\\\\n(TP + FP) &= \\frac{TP}{\\text{Precision}} \\\\\nFP &= \\frac{TP}{\\text{Precision}} - TP \\\\\nFP &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right)\n\\end{aligned}\n\\tag{3}\\]\nThe same algebraic process can be done for recall which gives,\n\\[\nFN = TP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right)\n\\tag{4}\\]\nNow looking back to our objective, to make the quantities of Type I and Type II errors equal, we should set \\(FP\\) and \\(FN\\) equal to each other.\n\\[\n\\begin{aligned}\nFN &= FP \\\\\nTP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right) &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right) \\\\\n\\frac{1}{\\text{Recall}} &= \\frac{1}{\\text{Precision}} \\\\\n\\text{Precision} &= \\text{Recall}\n\\end{aligned}\n\\tag{5}\\]\nSo we see that our error types will be equal in quantity when our precision and recall are equal."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "href": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "PR Curve",
    "text": "PR Curve\nNow that we know what we want to achieve, we have to write the code to do it.\nUsing pr_curve, we will shortcut a search of the possible threshold values ranging from zero to one.\n\nlibrary(dplyr)\n\npr &lt;- two_class_example |&gt; \n  as_tibble() |&gt; \n  pr_curve(truth, Class1)\n\nhead(pr)\n#&gt; # A tibble: 6 × 3\n#&gt;   .threshold  recall precision\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     Inf    0               1\n#&gt; 2       1.00 0.00388         1\n#&gt; 3       1.00 0.00775         1\n#&gt; 4       1.00 0.0116          1\n#&gt; 5       1.00 0.0155          1\n#&gt; 6       1.00 0.0194          1\n\nWe can plot the curve to find the point where precision and recall are equal.\n\nautoplot(pr) +\n  geom_abline(slope = 1, intercept = 0, color = \"grey\", linetype = \"dashed\")\n\n\n\n\n\n\nFigure 2: A PR curve for the two_class_example dataset.\n\n\n\n\nWe see that precision and recall are equal along the dashed line and this meets the PR curve at about (0.85, 0.85). The exact point would be available in pr.\n\npr_diff &lt;- pr |&gt; \n  mutate(diff = abs(precision - recall)) |&gt; \n  arrange(diff)\n\nhead(pr_diff)\n#&gt; # A tibble: 6 × 4\n#&gt;   .threshold recall precision    diff\n#&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1      0.610  0.864     0.864 0      \n#&gt; 2      0.618  0.860     0.864 0.00335\n#&gt; 3      0.602  0.868     0.865 0.00335\n#&gt; 4      0.601  0.868     0.862 0.00668\n#&gt; 5      0.618  0.857     0.863 0.00669\n#&gt; 6      0.590  0.868     0.858 0.00998\n\nthreshold &lt;- pr_diff |&gt; \n  slice(1) |&gt; \n  pull(.threshold)\n\nThis tells us that a threshold of 0.61 will provide us with a balanced number of positive cases. We can check this my looking at the confusion matrix when we use 0.61 as the threshold.\n\ntwo_class_adjusted &lt;- two_class_example |&gt; \n  mutate(\n    predicted_threshold = factor(\n      if_else(Class1 &gt;= threshold, \"Class1\", \"Class2\"),\n      levels = levels(two_class_example$truth)\n    )\n  )\n\ntwo_class_adjusted |&gt; \n  conf_mat(truth, predicted_threshold) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"lightblue\") +\n  coord_equal()\n\n\n\n\n\n\nFigure 3: The confusion matrix with the adjusted threshold value.\n\n\n\n\nNotice how now the lower-left and upper-right quadrants have equal values. This means that the number of observations classified as Class 1 matches the amount in the training data.\nHowever, this can come at the cost of reduced accuracy. Funnily enough this is not the case for this particular dataset, but it was for the dataset I was using.\n\nlibrary(tidyr)\n\ntwo_class_adjusted |&gt; \n  select(truth, predicted, predicted_threshold) |&gt; \n  pivot_longer(\n    cols            = c(predicted, predicted_threshold),\n    names_to        = \"threshold\",\n    values_to       = \"predicted\",\n    names_transform = \\(x) ifelse(x == \"predicted\", 0.5, threshold)\n  ) |&gt; \n  group_by(threshold) |&gt; \n  accuracy(truth, predicted) |&gt; \n  select(threshold, accuracy = .estimate)\n#&gt; # A tibble: 2 × 2\n#&gt;   threshold accuracy\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0.5      0.838\n#&gt; 2     0.610    0.86\n\nOf course, in reality you would want to find the best threshold for each training set and average them to apply to the test set."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Reading Log",
    "section": "",
    "text": "I have been focusing on reading important philosophical and psychological texts as well as textbooks pertaining to R, machine learning, and data science.\nI also listen to podcasts and lectures to center my mind on the important things in life. Centering around being better disciplined and having mental fortitude alongside physical strength and intelligence.\nDuring the academic year my reading frequency will probably slow to make room for assigned readings."
  },
  {
    "objectID": "books.html#section",
    "href": "books.html#section",
    "title": "Reading Log",
    "section": "2025",
    "text": "2025\nMy goal for this year is 12 books.\n\n\n\n\n\n\n\nPhilosophy and Human Geography (1986, 2nd edition) by Ron Johnston.\nThe Screwtape Letters (1942) by C. S. Lewis.\nAdvanced R (2019, 2nd edition) by Hadley Wickham.\nThe Two Towers (1994 edition) by J. R. R. Tolkien.\nStart, Stop, or Grow? (2022) by Robert Gray Atkins.\nTidy Modeling with R (2023) by Max Kuhn and Julia Silge.\nThe Fellowship of the Ring (1994 edition) by J. R. R. Tolkien.\nHands-On Machine Learning with R (2020) by Bradley Boehmke and Brandon Greenwell.\nMere Christianity (1952) by C. S. Lewis."
  },
  {
    "objectID": "books.html#section-1",
    "href": "books.html#section-1",
    "title": "Reading Log",
    "section": "2024",
    "text": "2024\nI started reading more deliberately in the fall.\n\nThe Bible - Most of the Old Testament I read in the NIV, but the rest I read in the ESV.\nAwesome Encounters with God: The Miracles of Jesus (2003) by Dr. Russel Bone.\nggplot2: Elegant Graphics for Data Analysis (3rd edition) by Wickham, Navarro and Pedersen.\nHillbilly Elegy (2016) by Vice President J. D. Vance.\nR for Data Science (2017, 2nd edition) by Hadley Wickham.\nThe American Story: The Beginnings (2020) by David and Tim Barton.\nThe Hobbit (1995 edition) by J. R. R. Tolkien."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nPhD in Spatially Integrated Social Science | August 2025 - Present (expected graduation: 2030)\nMA in Economics | May 2024\nCertificate of Data Analytics in Economics | May 2024\nBS in Data Science | May 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nPhD in Spatially Integrated Social Science | August 2025 - Present (expected graduation: 2030)\nMA in Economics | May 2024\nCertificate of Data Analytics in Economics | May 2024\nBS in Data Science | May 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "S. L. Carter",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Toledo | Toledo, Ohio\n\n\n\n\n\n\nBusiness Insights Analyst | May 2024 - Present\n\n\n\n\n\n\nProjects\n\nAnalyzed enrollment and admissions data to develop a detailed simulation and forecast of university enrollment.\nBuilt several Shiny applications for streamlining data analysis and visualization to better inform decision-makers in a dynamic and intuitive manner.\nDesigned a number of webscraping and data acquisition tools to ensure straightforward data sharing and analysis.\nProduced streamlined processes to download and transform data into language agnostic formats.\nDeveloped a check to search for unusual work history and flag for review.\n\n\n\nSpecialties\n\nR\n\ntidyverse, tidymodels, and fpp3 primarily, in addition to others.\n\nShiny for R and Python\nQuarto and RMarkdown\nSQL\nPython\n\n\n\n\n\n\n\n\n\n\n\nGraduate Assistant | Aug 2023 - May 2024\n\n\n\n\n\nSpecialties\n\nR\nShiny for R\nRMarkdown\n\n\n\n\n\n\n\n\n\n\nUndergraduate Research Assistant | Oct 2022 - Aug 2023\n\n\n\n\n\n\nProjects\n\nDeveloped software for performing high quality electrical measurements with efficiency and precision. Improved experimental speed and consistency through software improvements and advances to the physical experimentation system.\nDeveloped workflows to improve quality and consistency for analyzing experimental data output.\nPerformed electrical and optical photovoltaic measurements.\nSee below for papers in which the software I largely developed was utilized, even beyond my time of employment.\n\n\n\nSpecialties\n\nPython\n\nNumPy, scikit-learn, and pandas."
  },
  {
    "objectID": "index.html#sec-publications",
    "href": "index.html#sec-publications",
    "title": "S. L. Carter",
    "section": "Publications",
    "text": "Publications\nFor the 2024 and 2025 articles, I was a major contributer to the software used to perform the transient photovoltage/photocurrent experiments as well as developing scripts to automate initial analysis. For the 2023 article, contributed a Python script to automate a basic analysis of some data; a relatively small part of the project.\n\n2025\n\nAbasi Abudulimu, Sheng Fu, Nadeesha Katakumbura, Nannan Sun, Steven Carter, Tyler Brau, Lei Chen, Manoj Rajakaruna, Jared Friedl, Zhaoning Song, Adam B. Phillips, Michael J. Heben, Yanfa Yan and Randy J. Ellingson. 2025. “Enchancing Understanding of Recombination Mechanisms in High-Performance Tin-Lead Perovskite Solar Cells.” Cell Reports Physical Science 6 (1). https://doi.org/10.1016/j.xcrp.2024.102349.\n\n\n\n2024\n\nAbudulimu, Abasi, Adam Phillips, Steven Carter, Sabin Neupane, Deng-Bing Li, Tyler Brau, Jared Friedl, Yanfa Yan, Michael Heben, and Randy Ellingson. 2024. “Transient Photovoltage as a Tool for Probing Carrier Dynamics in CdTe-Based Solar Cells.” In 2024 IEEE 52nd Photovoltaic Specialist Conference (PVSC), 1220–20. IEEE. https://doi.org/10.1109/pvsc57443.2024.10749335.\nAbudulimu, Abasi, Steven Carter, Adam B. Phillips, Deng‐Bing Li, Sabin Neupane, Tyler Brau, Jared Friedl, et al. 2024. “Comprehensive Study of Carrier Recombination in High‐efficiency CdTe Solar Cells Using Transient Photovoltage.” Solar RRL 8 (10). https://doi.org/10.1002/solr.202400131.\n\n\n\n2023\n\nPhillips, Adam B., Jared D. Friedl, Philip Ottinger, Steven L. Carter, Zhaoning Song, Abasi Abudulimu, Ebin Bastola, et al. 2023. “Understanding and Advancing Bifacial Thin Film Solar Cells Under Dual Illumination.” Solar RRL 7 (21). https://doi.org/10.1002/solr.202300545."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html",
    "href": "posts/2025-03-01-data-sci-advice/index.html",
    "title": "Advice for Incoming Data Science Students",
    "section": "",
    "text": "Recently I had the opportunity to speak to current Data Science, Data Analytics, Economics, and Computer Science students. I wanted to collect some of the thoughts expressed by myself and the other panelists: Jared and Ranesh."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#markdown-and-latex",
    "href": "posts/2025-03-01-data-sci-advice/index.html#markdown-and-latex",
    "title": "Advice for Incoming Data Science Students",
    "section": "Markdown and \\(\\LaTeX\\)",
    "text": "Markdown and \\(\\LaTeX\\)\nMarkdown is a must in my mind. It is used for all kinds of formats (including this article). I use it every day for taking notes, writing reports, and writing documentation.\n\nMarkdown Guide\n\n\\(\\LaTeX\\) is an older tool used to create high quality documents. It is the language that produces textbooks and academic journals. I would recommend becoming somewhat familiar with, especially the math formatting.\n\nOverleaf is a good site to help you get exposed to \\(\\LaTeX\\).\n\nIf you could only learn one, pick Markdown, but knowing both is beneficial."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#r",
    "href": "posts/2025-03-01-data-sci-advice/index.html#r",
    "title": "Advice for Incoming Data Science Students",
    "section": "R",
    "text": "R\nR is great for statistics. That is its main selling point. Base R is not the best in my opinion, but the community is very active and has produced a ton of high quality packages which make R incredibly approachable.\nThe two primary packages for handling tables are {tidyverse} and {data.table}.\n\n{tidyverse}\nThis is a collection of packages that are unified in their design. These are great for ease of use, but can be a little slow for large operations. This based around {tibble}. All of the functions in this have a similar style and can be similar to plain English.\nFor modeling there is the {tidymodels} ecosystem. This collects many different models from a variety of packages under a consistent framework.\n\n\n{data.table}\nThis package is similar in purpose to main {tidyverse} packages of {dplyr} and {tidyr}, but is built to be more memory and time efficient. The syntax is more similar to base R and in my opinion is more difficult than the {tidyverse}.\nI only really use this when I need something to be fast, favoring the {tidyverse} for regular operations.\n\n\nShiny\nR Shiny is an awesome tool to build interactive dashboards, reports, webpages, etc.\nCombining the interactive power of this with {plotly}, can produce very nice dashboards with a high level of customization.\n\n\nOnline Resources\nThe R community has put together a plethora of free textbooks and guides.\nI have read these ones so far, I’ve ordered them by how difficult I feel the content is:\n\nR for Data Science by Hadley Wickham.\nggplot2: Elegant Graphics for Data Analysis by Wickham, Navarro and Pedersen.\nTidy Modeling with R by Kuhn & Silge.\nHands-On Machine Learning with R by Boehmke & Greenwell.\n\nOthers that I have read pieces of, or I think would be good to read:\n\nR Cookbook by Teetor.\nForecasting: Principles and Practice by Hyndman and Athanasopoulos.\nMastering Shiny by Wickham.\nAdvanced R by Wickham.\nR Packages by Wickham & Bryan.\n\nThere are tons of other books available. The Big Book of R lists probably close to all of them."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#python",
    "href": "posts/2025-03-01-data-sci-advice/index.html#python",
    "title": "Advice for Incoming Data Science Students",
    "section": "Python",
    "text": "Python\nI haven’t used Python as much recently, but it is similar to R, but not as focused on statistics. Python has more ability for general programming needs. It is, however, the go-to language for many when doing machine learning.\nThe main libraries for Python are NumPy, matplotlib, and pandas. More recently a competitor to pandas has emerged: polars. For machine learning and modeling there are tensorflow, pytorch, kerars, and SciPy.\nShiny and plotly are also available for Python.\n\n\n\n\n\n\nOpinion\n\n\n\nI am not really a fan of the documenation style of Python compared to R. To me to R docs are very consistent thanks to {roxygen2}, but the Python docs are more clutered.\n\n\nThere are a number of tutorial websites and videos available. A few of resources mentioned during the Q&A panel were Kaggle, DataCamp, W3 Schools (they have lots of languages available), and StackOverflow (which has Q&A content for all kinds of programming topics)."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#quarto",
    "href": "posts/2025-03-01-data-sci-advice/index.html#quarto",
    "title": "Advice for Incoming Data Science Students",
    "section": "Quarto",
    "text": "Quarto\nI have used Quarto almost daily for many months. It is incredibly useful. It is similar to Jupyter. I’m not up to date on what Jupyter can do, but Quarto can use R, Python, Julia, and ObservableJS. It is written in plain text and can take advantage of \\(\\LaTeX\\) and Markdown with some additional features. It can render to HTML, PDF, Word, and other formats including slideshows. You can even create books (such as many of the ones listed above) and websites (like this one) using Quarto.\nI would recommend using this for homework assignments and projects if the professor allows."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#other-cool-stuff",
    "href": "posts/2025-03-01-data-sci-advice/index.html#other-cool-stuff",
    "title": "Advice for Incoming Data Science Students",
    "section": "Other Cool Stuff",
    "text": "Other Cool Stuff\n\nDuckDB - A fairly new and handy DBMS that integrates nicely with R, Python, and others.\nObsidian - A good notetaking software that uses Markdown formatting. I used this for my last two years in college.\nLinux - If you want to broaden your computer skills, learn about Linux. Buy a Raspberry Pi (or mini computer) and start tinkering. Or, if you have an old laptop, install Mint or Pop!_OS (I have used both). Windows 10 is reaching end of support in October 2025. It may be time to jump ship and embrace the penguin.\nDocker - This can help deploy apps locally or on remote servers. I use this with my Raspberry Pi and at work."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Distance Calculations Lab Using R\n\n\n\nDistance\n\nR\n\nSpatial\n\n\n\n\n\n\n\nSteven Carter\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nDifferent Methods for Extracting JSON with R\n\n\n\nR\n\nJSON\n\n\n\n\n\n\n\nSteven Carter\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nAdvice for Incoming Data Science Students\n\n\n\nEducation\n\n\n\n\n\n\n\nSteven Carter\n\n\nMar 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nBalancing Classification Errors for Aggregate Consistency\n\n\n\nClassification\n\nR\n\nLogistic Regression\n\n\n\n\n\n\n\nSteven Carter\n\n\nNov 29, 2024\n\n\n\n\n\n\nNo matching items\n Back to top"
  }
]