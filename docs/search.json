[
  {
    "objectID": "tech.html",
    "href": "tech.html",
    "title": "Technologies I Use",
    "section": "",
    "text": "For electronic notes I use Obsidian. With my Bible notes I backup the Obsidian vault with git to a Gitea instance attached to a Raspberry Pi on my local network.\nFor physical notetaking I like to use a leatherbound notepad. Any notes I take from Bible studies I try to copy into the Obsidian vault."
  },
  {
    "objectID": "tech.html#languages",
    "href": "tech.html#languages",
    "title": "Technologies I Use",
    "section": "Languages",
    "text": "Languages\nCurrently I use R and Markdown/Quarto primarily and Python when R can’t do the job. In association with R and Python, I use SQL both directly and through {dplyr} backends.\n\n\n\nLanguage\nProficiency\nExperience\n\n\n\n\nR\nStrong\n2 years\n\n\nMarkdown, Quarto\nStrong\n2 years\n\n\nLaTeX\nModerate\n1 year\n\n\nPython\nModerate\n1 year\n\n\nJava\nBasic\n2 Semesters\n\n\nSQL\nModerate\n&lt;1 year\n\n\nHTML\nBasic\nMinimal\n\n\nCSS\nBasic\nMinimal\n\n\nDAX\nBasic\nMinimal\n\n\nJavaScript\nNear Zero\nNext to none"
  },
  {
    "objectID": "tech.html#software",
    "href": "tech.html#software",
    "title": "Technologies I Use",
    "section": "Software",
    "text": "Software\n\nRStudio: This is my go-to editor for writing reports and apps with Quarto and Shiny. The open-source community is really thriving and makes the R language really approachable.\nVS Code: I use this for miscellaneous code editing and any Python work. It is a good editor, but doesn’t fit into my usual workflow.\nPower BI: I am not a fan of any Microsoft data analysis software. It feels restrictive to do anything outside of basic charts and graphs. I always feel that I am fighting the program to get what I want. Being locked down in closed-source, paid ecosystem is a big negative. The documentation is severly lacking compared to that of the R community.\nTableau: Similar thoughts to Power BI, but I have generally had a better experience. However, I haven’t used it as much.\nArcGIS: I only used this for a class. It was an alright experience, but I haven’t done any complex work to feel out the program yet. All of my mapping needs recently have been filled by {plotly} and {ggplot2}.\nObsidian: This is a good and adaptable notetaking software.\nDuckDB: This is a good database software. I used it extensively in January 2025 and had no performance issue or major problems."
  },
  {
    "objectID": "posts/2025-03-06-json-extraction/index.html",
    "href": "posts/2025-03-06-json-extraction/index.html",
    "title": "Different Methods for Extracting JSON with R",
    "section": "",
    "text": "For the past month or so I have been working with nested JSON data. I wanted to find the fastest strategy for data extraction. My initial attempt was using the tidyverse. I was put off by the time this took to run. Next, I tried using data.table which I am fairly unfamiliar with.\nI was reading up on other methods and came across a post from Martin Morgan. He is the author of rjsoncons. His post covered the use of DuckDB for reading JSON which was quite performant.\nWhile sick today I went through the same operation with each approach.\n\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(data.table)\nlibrary(jsonlite)\n\nThe JSON file I’ll be using is from a NCAA basketball game and is located here. I’m not sure how the performance of these different approaches will vary depending on the structure of the JSON file.\n\nif (!file.exists(\"boxscore.json\")) {\n    download.file(\n        \"https://data.ncaa.com/casablanca/game/6351263/boxscore.json\", \n        destfile = \"boxscore.json\"\n    )\n}\n\n\n{tidyverse} Approach\nThis is the easiest to understand method in my opinion. It simply reads the JSON file using jsonlite, pulls out some metadata about the teams, pulls out some player statistics, and joins them together. It returns a tibble.\n\ntidyverse_style &lt;- function() {\n    j &lt;- fromJSON(\"boxscore.json\")\n\n    stats &lt;- j$teams |&gt; \n        tibble() |&gt; \n        select(teamId, playerStats) |&gt; \n        unnest(playerStats) |&gt; \n        mutate(teamId = as.character(teamId))\n  \n    full_join(\n        j$meta$teams, \n        stats, \n        by = join_by(id == teamId)\n    )\n}\n\n\n{data.table} Approach\nThe process here is essentially the same as above. This however returns a data.frame.\n\ndata.table_style &lt;- function() {\n    j &lt;- fromJSON(\"boxscore.json\")\n\n    teams &lt;- data.table(j$teams)[, .(\n        teamId = as.character(teamId),\n        rbindlist(playerStats)\n    )]\n  \n    return(\n        merge(\n            j$meta$teams,\n            teams, \n            by.x = \"id\",\n            by.y = \"teamId\"\n        ) |&gt; \n            setnames(\"id\", \"teamId\")\n    )\n}\n\nDuckDB Approach\nFor DuckDB we need to load up an in-memory database. Also, we have to install and load the JSON extension for DuckDB.\nThis also returns a data.frame.\n\ncon &lt;- dbConnect(duckdb())\n\ndbExecute(con, \"INSTALL json\")\ndbExecute(con, \"LOAD json\")\n\n\nduck_style &lt;- function() {\n    dbGetQuery(\n        con, \n        \"\n        with raw as (select meta, teams from 'boxscore.json'),\n        meta as (select unnest(meta.teams, recursive := true) from raw),\n        teams_large as (select unnest(teams) as team from raw),\n        teams as (\n            select\n                team.teamId::VARCHAR as teamId,\n                unnest(team.playerStats, recursive := true)\n            from teams_large\n        )\n        select * exclude meta.id\n        from meta, teams\n        where meta.id = teams.teamId\n        \"\n    )\n}\n\nBenchmark\n\nresults &lt;- microbenchmark::microbenchmark(\n  duck  = duck_style(),\n  tidy  = tidyverse_style(),\n  dt    = data.table_style(),\n  times = 100\n)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn my testing with a standard R script, DuckDB came out about 7ms faster than the tidy approach. I’m not sure what the difference is.\n\n\nWe see that data.table is the clear winner here with times below 10ms. DuckDB looks to be the most consistent.\nThere are probably some more optimizations that I am not seeing, but this is what I can up with in a few hours."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html",
    "href": "posts/2024-11-28-classification-errors/index.html",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "",
    "text": "Recently, I was developing a model to predict a binary outcome. I settled on a logistic regression model since that was something I was somewhat familiar with from my Econometrics class. From a logistic regression model you estimate the probability of the observation belonging to the positive class. As one would guess, a probability greater than 50% would indicate the positive class, and below 50%, the negative class. This is the threshold value; the threshold where the classification changes from one outcome to the other.\n\\[\n\\hat{y_i} = \\begin{cases}\np(x_i) \\geq t, &\\text{then } 1 \\\\\np(x_i) &lt; t, &\\text{then } 0\n\\end{cases}\n\\tag{1}\\] where \\(t\\) is the threshold value and \\(t\\in[0,1]\\)."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "href": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nThere are many different websites that provide a definition for these metrics. You can easily find several detailed explanations with a quick search.\n\\[\n\\begin{split}\n\\text{Precision}&= \\frac{\\text{True Positive}}{\\text{True Positive}+ \\text{False Positive}} = \\frac{TP}{TP+FP} \\\\\n\\text{Recall} &= \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{TP}{TP+FN}\n\\end{split}\n\\tag{2}\\]\nIf we rearrange these definitions to solve for \\(FP\\) and \\(FN\\), we can make some progress.\n\\[\n\\begin{aligned}\n\\text{Precision}\\cdot(TP+FP) &= TP \\\\\n(TP + FP) &= \\frac{TP}{\\text{Precision}} \\\\\nFP &= \\frac{TP}{\\text{Precision}} - TP \\\\\nFP &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right)\n\\end{aligned}\n\\tag{3}\\]\nThe same algebraic process can be done for recall which gives,\n\\[\nFN = TP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right)\n\\tag{4}\\]\nNow looking back to our objective, to make the quantities of Type I and Type II errors equal, we should set \\(FP\\) and \\(FN\\) equal to each other.\n\\[\n\\begin{aligned}\nFN &= FP \\\\\nTP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right) &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right) \\\\\n\\frac{1}{\\text{Recall}} &= \\frac{1}{\\text{Precision}} \\\\\n\\text{Precision} &= \\text{Recall}\n\\end{aligned}\n\\tag{5}\\]\nSo we see that our error types will be equal in quantity when our precision and recall are equal."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "href": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "PR Curve",
    "text": "PR Curve\nNow that we know what we want to achieve, we have to write the code to do it.\nUsing pr_curve, we will shortcut a search of the possible threshold values ranging from zero to one.\n\nlibrary(dplyr)\n\npr &lt;- two_class_example |&gt; \n  as_tibble() |&gt; \n  pr_curve(truth, Class1)\n\nhead(pr)\n#&gt; # A tibble: 6 × 3\n#&gt;   .threshold  recall precision\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     Inf    0               1\n#&gt; 2       1.00 0.00388         1\n#&gt; 3       1.00 0.00775         1\n#&gt; 4       1.00 0.0116          1\n#&gt; 5       1.00 0.0155          1\n#&gt; 6       1.00 0.0194          1\n\nWe can plot the curve to find the point where precision and recall are equal.\n\nautoplot(pr) +\n  geom_abline(slope = 1, intercept = 0, color = \"grey\", linetype = \"dashed\")\n\n\n\n\n\n\nFigure 2: A PR curve for the two_class_example dataset.\n\n\n\n\nWe see that precision and recall are equal along the dashed line and this meets the PR curve at about (0.85, 0.85). The exact point would be available in pr.\n\npr_diff &lt;- pr |&gt; \n  mutate(diff = abs(precision - recall)) |&gt; \n  arrange(diff)\n\nhead(pr_diff)\n#&gt; # A tibble: 6 × 4\n#&gt;   .threshold recall precision    diff\n#&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1      0.610  0.864     0.864 0      \n#&gt; 2      0.618  0.860     0.864 0.00335\n#&gt; 3      0.602  0.868     0.865 0.00335\n#&gt; 4      0.601  0.868     0.862 0.00668\n#&gt; 5      0.618  0.857     0.863 0.00669\n#&gt; 6      0.590  0.868     0.858 0.00998\n\nthreshold &lt;- pr_diff |&gt; \n  slice(1) |&gt; \n  pull(.threshold)\n\nThis tells us that a threshold of 0.61 will provide us with a balanced number of positive cases. We can check this my looking at the confusion matrix when we use 0.61 as the threshold.\n\ntwo_class_adjusted &lt;- two_class_example |&gt; \n  mutate(\n    predicted_threshold = factor(\n      if_else(Class1 &gt;= threshold, \"Class1\", \"Class2\"),\n      levels = levels(two_class_example$truth)\n    )\n  )\n\ntwo_class_adjusted |&gt; \n  conf_mat(truth, predicted_threshold) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"lightblue\") +\n  coord_equal()\n\n\n\n\n\n\nFigure 3: The confusion matrix with the adjusted threshold value.\n\n\n\n\nNotice how now the lower-left and upper-right quadrants have equal values. This means that the number of observations classified as Class 1 matches the amount in the training data.\nHowever, this can come at the cost of reduced accuracy. Funnily enough this is not the case for this particular dataset, but it was for the dataset I was using.\n\nlibrary(tidyr)\n\ntwo_class_adjusted |&gt; \n  select(truth, predicted, predicted_threshold) |&gt; \n  pivot_longer(\n    cols            = c(predicted, predicted_threshold),\n    names_to        = \"threshold\",\n    values_to       = \"predicted\",\n    names_transform = \\(x) ifelse(x == \"predicted\", 0.5, threshold)\n  ) |&gt; \n  group_by(threshold) |&gt; \n  accuracy(truth, predicted) |&gt; \n  select(threshold, accuracy = .estimate)\n#&gt; # A tibble: 2 × 2\n#&gt;   threshold accuracy\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0.5      0.838\n#&gt; 2     0.610    0.86\n\nOf course, in reality you would want to find the best threshold for each training set and average them to apply to the test set."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Reading Log",
    "section": "",
    "text": "I have been focusing on reading important philosophical and psychological texts as well as textbooks pertaining to R, machine learning and data science.\nI also listen to podcasts and lectures to center my mind on the important things in life. Centering around being better disciplined and having mental fortitude alongside physical strength and intelligence."
  },
  {
    "objectID": "books.html#section",
    "href": "books.html#section",
    "title": "Reading Log",
    "section": "2025",
    "text": "2025\nMy goal for this year is 12 books.\n\n\n\n\n\n\n\nTidy Modeling with R (2023) by Max Kuhn and Julia Silge.\nThe Fellowship of the Ring (1994 edition) by J. R. R. Tolkien.\nHands-On Machine Learning with R (2020) by Bradley Boehmke and Brandon Greenwell.\nMere Christianity (1952) by C. S. Lewis."
  },
  {
    "objectID": "books.html#section-1",
    "href": "books.html#section-1",
    "title": "Reading Log",
    "section": "2024",
    "text": "2024\nI started reading more deliberately in the fall.\n\nThe Bible - Most of the Old Testament I read in the NIV, but the rest I read in the ESV.\nAwesome Encounters with God: The Miracles of Jesus (2003) by Dr. Russel Bone.\nggplot2: Elegant Graphics for Data Analysis (3rd edition) by Wickham, Navarro and Pedersen.\nHillbilly Elegy (2016) by Vice President J. D. Vance.\nR for Data Science (2nd edition, 2017) by Hadley Wickham.\nThe American Story: The Beginnings (2020) by David and Tim Barton.\nThe Hobbit (1995 edition) by J. R. R. Tolkien."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nMA in Economics | August 2023 - May 2024\nCertificate of Data Analytics in Economics | August 2023 - May 2024\nBS in Data Science | August 2020 - May 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nMA in Economics | August 2023 - May 2024\nCertificate of Data Analytics in Economics | August 2023 - May 2024\nBS in Data Science | August 2020 - May 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "S. L. Carter",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Toledo | Toledo, Ohio\n\n\n\n\n\n\nBusiness Insights Analyst | May 2024 - Present\n\n\n\n\n\n\nProjects\n\nAnalyzed enrollment and admissions data to develop a detailed simulation and forecast of university enrollment.\nBuilt several Shiny applications for streamlining data analysis and visualization to better inform decision-makers in a dynamic and intuitive manner.\nDesigned a number of webscraping and data aquisition tools to ensure straightforward data sharing and analysis.\nProduced a streamlined process to download and transform public educational data into a language agnostic format.\nDeveloped a check to search for unusual work history and flag for review.\n\n\n\nSpecialties\n\nR\n\nTidyverse, tidymodels, and fpp3 primarily, in addition to others.\n\nShiny for R\nQuarto and RMarkdown\nSQL\n\n\n\n\n\n\n\n\n\n\n\nGraduate Assistant | Aug 2023 - May 2024\n\n\n\n\n\nSpecialties\n\nR\nShiny for R\nRMarkdown\n\n\n\n\n\n\n\n\n\n\nUndergraduate Research Assistant | Oct 2022 - Aug 2023\n\n\n\n\n\n\nProjects\n\nDeveloped software for performing high quality electrical measurements with efficiency and precision. Improved experimental speed and consistency through software improvements and advances to the physical experimentation system.\nDeveloped workflows to improve quality and consistency for analyzing experimental data output.\nPerformed electrical and optical photovoltaic measurements.\nSee below for papers in which the software I largely developed was utilized, even beyond my time of employment.\n\n\n\nSpecialties\n\nPython\n\nNumPy, scikit-learn, and pandas."
  },
  {
    "objectID": "index.html#sec-publications",
    "href": "index.html#sec-publications",
    "title": "S. L. Carter",
    "section": "Publications",
    "text": "Publications\nFor the 2024 and 2025 articles, I was a major contributer to the software used to perform the transient photovoltage/photocurrent experiments as well as developing scripts to automate initial analysis. For the 2023 article, contributed a Python script to automate a basic analysis of some data; a relatively small part of the project.\n\n2025\n\nAbasi Abudulimu, Sheng Fu, Nadeesha Katakumbura, Nannan Sun, Steven Carter, Tyler Brau, Lei Chen, Manoj Rajakaruna, Jared Friedl, Zhaoning Song, Adam B. Phillips, Michael J. Heben, Yanfa Yan and Randy J. Ellingson. 2025. “Enchancing Understanding of Recombination Mechanisms in High-Performance Tin-Lead Perovskite Solar Cells.” Cell Reports Physical Science 6 (1). https://doi.org/10.1016/j.xcrp.2024.102349.\n\n\n\n2024\n\nAbudulimu, Abasi, Adam Phillips, Steven Carter, Sabin Neupane, Deng-Bing Li, Tyler Brau, Jared Friedl, Yanfa Yan, Michael Heben, and Randy Ellingson. 2024. “Transient Photovoltage as a Tool for Probing Carrier Dynamics in CdTe-Based Solar Cells.” In 2024 IEEE 52nd Photovoltaic Specialist Conference (PVSC), 1220–20. IEEE. https://doi.org/10.1109/pvsc57443.2024.10749335.\nAbudulimu, Abasi, Steven Carter, Adam B. Phillips, Deng‐Bing Li, Sabin Neupane, Tyler Brau, Jared Friedl, et al. 2024. “Comprehensive Study of Carrier Recombination in High‐efficiency CdTe Solar Cells Using Transient Photovoltage.” Solar RRL 8 (10). https://doi.org/10.1002/solr.202400131.\n\n\n\n2023\n\nPhillips, Adam B., Jared D. Friedl, Philip Ottinger, Steven L. Carter, Zhaoning Song, Abasi Abudulimu, Ebin Bastola, et al. 2023. “Understanding and Advancing Bifacial Thin Film Solar Cells Under Dual Illumination.” Solar RRL 7 (21). https://doi.org/10.1002/solr.202300545."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html",
    "href": "posts/2025-03-01-data-sci-advice/index.html",
    "title": "Advice for Incoming Data Science Students",
    "section": "",
    "text": "Recently I had the opportunity to speak to current Data Science, Data Analytics, Economics, and Computer Science students. I wanted to collect some of the thoughts expressed by myself and the other panelists: Jared and Ranesh."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#markdown-and-latex",
    "href": "posts/2025-03-01-data-sci-advice/index.html#markdown-and-latex",
    "title": "Advice for Incoming Data Science Students",
    "section": "Markdown and \\(\\LaTeX\\)",
    "text": "Markdown and \\(\\LaTeX\\)\nMarkdown is a must in my mind. It is used for all kinds of formats (including this article). I use it every day for taking notes, writing reports, and writing documentation.\n\nMarkdown Guide\n\n\\(\\LaTeX\\) is an older tool used to create high quality documents. It is the language that produces textbooks and academic journals. I would recommend becoming somewhat familiar with, especially the math formatting.\n\nOverleaf is a good site to help you get exposed to \\(\\LaTeX\\).\n\nIf you could only learn one, pick Markdown, but knowing both is beneficial."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#r",
    "href": "posts/2025-03-01-data-sci-advice/index.html#r",
    "title": "Advice for Incoming Data Science Students",
    "section": "R",
    "text": "R\nR is great for statistics. That is its main selling point. Base R is not the best in my opinion, but the community is very active and has produced a ton of high quality packages which make R incredibly approachable.\nThe two primary packages for handling tables are {tidyverse} and {data.table}.\n\n{tidyverse}\nThis is a collection of packages that are unified in their design. These are great for ease of use, but can be a little slow for large operations. This based around {tibble}. All of the functions in this have a similar style and can be similar to plain English.\nFor modeling there is the {tidymodels} ecosystem. This collects many different models from a variety of packages under a consistent framework.\n\n\n{data.table}\nThis package is similar in purpose to main {tidyverse} packages of {dplyr} and {tidyr}, but is built to be more memory and time efficient. The syntax is more similar to base R and in my opinion is more difficult than the {tidyverse}.\nI only really use this when I need something to be fast, favoring the {tidyverse} for regular operations.\n\n\nShiny\nR Shiny is an awesome tool to build interactive dashboards, reports, webpages, etc.\nCombining the interactive power of this with {plotly}, can produce very nice dashboards with a high level of customization.\n\n\nOnline Resources\nThe R community has put together a plethora of free textbooks and guides.\nI have read these ones so far, I’ve ordered them by how difficult I feel the content is:\n\nR for Data Science by Hadley Wickham.\nggplot2: Elegant Graphics for Data Analysis by Wickham, Navarro and Pedersen.\nTidy Modeling with R by Kuhn & Silge.\nHands-On Machine Learning with R by Boehmke & Greenwell.\n\nOthers that I have read pieces of, or I think would be good to read:\n\nR Cookbook by Teetor.\nForecasting: Principles and Practice by Hyndman and Athanasopoulos.\nMastering Shiny by Wickham.\nAdvanced R by Wickham.\nR Packages by Wickham & Bryan.\n\nThere are tons of other books available. The Big Book of R lists probably close to all of them."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#python",
    "href": "posts/2025-03-01-data-sci-advice/index.html#python",
    "title": "Advice for Incoming Data Science Students",
    "section": "Python",
    "text": "Python\nI haven’t used Python as much recently, but it is similar to R, but not as focused on statistics. Python has more ability for general programming needs. It is, however, the go-to language for many when doing machine learning.\nThe main libraries for Python are NumPy, matplotlib, and pandas. More recently a competitor to pandas has emerged: polars. For machine learning and modeling there are tensorflow, pytorch, kerars, and SciPy.\nShiny and plotly are also available for Python.\n\n\n\n\n\n\nOpinion\n\n\n\nI am not really a fan of the documenation style of Python compared to R. To me to R docs are very consistent thanks to {roxygen2}, but the Python docs are more clutered.\n\n\nThere are a number of tutorial websites and videos available. A few of resources mentioned during the Q&A panel were Kaggle, DataCamp, W3 Schools (they have lots of languages available), and StackOverflow (which has Q&A content for all kinds of programming topics)."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#quarto",
    "href": "posts/2025-03-01-data-sci-advice/index.html#quarto",
    "title": "Advice for Incoming Data Science Students",
    "section": "Quarto",
    "text": "Quarto\nI have used Quarto almost daily for many months. It is incredibly useful. It is similar to Jupyter. I’m not up to date on what Jupyter can do, but Quarto can use R, Python, Julia, and ObservableJS. It is written in plain text and can take advantage of \\(\\LaTeX\\) and Markdown with some additional features. It can render to HTML, PDF, Word, and other formats including slideshows. You can even create books (such as many of the ones listed above) and websites (like this one) using Quarto.\nI would recommend using this for homework assignments and projects if the professor allows."
  },
  {
    "objectID": "posts/2025-03-01-data-sci-advice/index.html#other-cool-stuff",
    "href": "posts/2025-03-01-data-sci-advice/index.html#other-cool-stuff",
    "title": "Advice for Incoming Data Science Students",
    "section": "Other Cool Stuff",
    "text": "Other Cool Stuff\n\nDuckDB - A fairly new and handy DBMS that integrates nicely with R, Python, and others.\nObsidian - A good notetaking software that uses Markdown formatting. I used this for my last two years in college.\nLinux - If you want to broaden your computer skills, learn about Linux. Buy a Raspberry Pi (or mini computer) and start tinkering. Or, if you have an old laptop, install Mint or Pop!_OS (I have used both). Windows 10 is reaching end of support in October 2025. It may be time to jump ship and embrace the penguin.\nDocker - This can help deploy apps locally or on remote servers. I use this with my Raspberry Pi and at work."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Different Methods for Extracting JSON with R\n\n\n\n\n\n\nR\n\n\nJSON\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\nSteven Carter\n\n\n\n\n\n\n\n\n\n\n\n\nAdvice for Incoming Data Science Students\n\n\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\n\nMar 1, 2025\n\n\nSteven Carter\n\n\n\n\n\n\n\n\n\n\n\n\nBalancing Classification Errors for Aggregate Consistency\n\n\n\n\n\n\nClassification\n\n\nR\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\nNov 29, 2024\n\n\nSteven Carter\n\n\n\n\n\n\nNo matching items"
  }
]