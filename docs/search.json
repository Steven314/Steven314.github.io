[
  {
    "objectID": "tech.html",
    "href": "tech.html",
    "title": "Technologies I Use",
    "section": "",
    "text": "For electronic notes I use Obsidian. With my Bible notes I backup the Obsidian vault with git to a Gitea instance attached to a Raspberry Pi on my local network.\nFor physical notetaking I like to use a leatherbound notepad. Any notes I take from Bible studies I try to copy into the Obsidian vault."
  },
  {
    "objectID": "tech.html#languages",
    "href": "tech.html#languages",
    "title": "Technologies I Use",
    "section": "Languages",
    "text": "Languages\nCurrently I use R and Markdown/Quarto primarily and Python when R can’t do the job. In association with R and Python, I use SQL both directly and through {dplyr} backends.\n\n\n\nLanguage\nProficiency\nExperience\n\n\n\n\nR\nStrong\n2 years\n\n\nMarkdown, Quarto\nStrong\n2 years\n\n\nLaTeX\nModerate\n1 year\n\n\nPython\nModerate\n1 year\n\n\nJava\nBasic\n2 Semesters\n\n\nSQL\nModerate\n&lt;1 year\n\n\nHTML\nBasic\nMinimal\n\n\nCSS\nBasic\nMinimal\n\n\nDAX\nBasic\nMinimal\n\n\nJavaScript\nNear Zero\nNext to none"
  },
  {
    "objectID": "tech.html#software",
    "href": "tech.html#software",
    "title": "Technologies I Use",
    "section": "Software",
    "text": "Software\n\nRStudio: This is my go-to editor for writing reports and apps with Quarto and Shiny. The open-source community is really thriving and makes the R language really approachable.\nVS Code: I use this for miscellaneous code editing and any Python work. It is a good editor, but doesn’t fit into my usual workflow.\nPower BI: I am not a fan of any Microsoft data analysis software. It feels restrictive to do anything outside of basic charts and graphs. I always feel that I am fighting the program to get what I want. Being locked down in closed-source, paid ecosystem is a big negative. The documentation is severly lacking compared to that of the R community.\nTableau: Similar thoughts to Power BI, but I have generally had a better experience. However, I haven’t used it as much.\nArcGIS: I only used this for a class. It was an alright experience, but I haven’t done any complex work to feel out the program yet. All of my mapping needs recently have been filled by {plotly} and {ggplot2}.\nObsidian: This is a good and adaptable notetaking software.\nDuckDB: This is a good database software. I used it extensively in January 2025 and had no performance issue or major problems."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html",
    "href": "posts/2024-11-28-classification-errors/index.html",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "",
    "text": "Recently, I was developing a model to predict a binary outcome. I settled on a logistic regression model since that was something I was somewhat familiar with from my Econometrics class. From a logistic regression model you estimate the probability of the observation belonging to the positive class. As one would guess, a probability greater than 50% would indicate the positive class, and below 50%, the negative class. This is the threshold value; the threshold where the classification changes from one outcome to the other.\n\\[\n\\hat{y_i} = \\begin{cases}\np(x_i) \\geq t, &\\text{then } 1 \\\\\np(x_i) &lt; t, &\\text{then } 0\n\\end{cases}\n\\tag{1}\\] where \\(t\\) is the threshold value and \\(t\\in[0,1]\\)."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "href": "posts/2024-11-28-classification-errors/index.html#precision-and-recall",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nThere are many different websites that provide a definition for these metrics. You can easily find several detailed explanations with a quick search.\n\\[\n\\begin{split}\n\\text{Precision}&= \\frac{\\text{True Positive}}{\\text{True Positive}+ \\text{False Positive}} = \\frac{TP}{TP+FP} \\\\\n\\text{Recall} &= \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{TP}{TP+FN}\n\\end{split}\n\\tag{2}\\]\nIf we rearrange these definitions to solve for \\(FP\\) and \\(FN\\), we can make some progress.\n\\[\n\\begin{aligned}\n\\text{Precision}\\cdot(TP+FP) &= TP \\\\\n(TP + FP) &= \\frac{TP}{\\text{Precision}} \\\\\nFP &= \\frac{TP}{\\text{Precision}} - TP \\\\\nFP &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right)\n\\end{aligned}\n\\tag{3}\\]\nThe same algebraic process can be done for recall which gives,\n\\[\nFN = TP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right)\n\\tag{4}\\]\nNow looking back to our objective, to make the quantities of Type I and Type II errors equal, we should set \\(FP\\) and \\(FN\\) equal to each other.\n\\[\n\\begin{aligned}\nFN &= FP \\\\\nTP \\cdot \\left(\\frac{1}{\\text{Recall}} - 1\\right) &= TP \\cdot \\left(\\frac{1}{\\text{Precision}} - 1\\right) \\\\\n\\frac{1}{\\text{Recall}} &= \\frac{1}{\\text{Precision}} \\\\\n\\text{Precision} &= \\text{Recall}\n\\end{aligned}\n\\tag{5}\\]\nSo we see that our error types will be equal in quantity when our precision and recall are equal."
  },
  {
    "objectID": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "href": "posts/2024-11-28-classification-errors/index.html#pr-curve",
    "title": "Balancing Classification Errors for Aggregate Consistency",
    "section": "PR Curve",
    "text": "PR Curve\nNow that we know what we want to achieve, we have to write the code to do it.\nUsing pr_curve, we will shortcut a search of the possible threshold values ranging from zero to one.\n\nlibrary(dplyr)\n\npr &lt;- two_class_example |&gt; \n  as_tibble() |&gt; \n  pr_curve(truth, Class1)\n\nhead(pr)\n#&gt; # A tibble: 6 × 3\n#&gt;   .threshold  recall precision\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     Inf    0               1\n#&gt; 2       1.00 0.00388         1\n#&gt; 3       1.00 0.00775         1\n#&gt; 4       1.00 0.0116          1\n#&gt; 5       1.00 0.0155          1\n#&gt; 6       1.00 0.0194          1\n\nWe can plot the curve to find the point where precision and recall are equal.\n\nautoplot(pr) +\n  geom_abline(slope = 1, intercept = 0, color = \"grey\", linetype = \"dashed\")\n\n\n\n\n\n\nFigure 2: A PR curve for the two_class_example dataset.\n\n\n\n\nWe see that precision and recall are equal along the dashed line and this meets the PR curve at about (0.85, 0.85). The exact point would be available in pr.\n\npr_diff &lt;- pr |&gt; \n  mutate(diff = abs(precision - recall)) |&gt; \n  arrange(diff)\n\nhead(pr_diff)\n#&gt; # A tibble: 6 × 4\n#&gt;   .threshold recall precision    diff\n#&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1      0.610  0.864     0.864 0      \n#&gt; 2      0.618  0.860     0.864 0.00335\n#&gt; 3      0.602  0.868     0.865 0.00335\n#&gt; 4      0.601  0.868     0.862 0.00668\n#&gt; 5      0.618  0.857     0.863 0.00669\n#&gt; 6      0.590  0.868     0.858 0.00998\n\nthreshold &lt;- pr_diff |&gt; \n  slice(1) |&gt; \n  pull(.threshold)\n\nThis tells us that a threshold of 0.61 will provide us with a balanced number of positive cases. We can check this my looking at the confusion matrix when we use 0.61 as the threshold.\n\ntwo_class_adjusted &lt;- two_class_example |&gt; \n  mutate(\n    predicted_threshold = factor(\n      if_else(Class1 &gt;= threshold, \"Class1\", \"Class2\"),\n      levels = levels(two_class_example$truth)\n    )\n  )\n\ntwo_class_adjusted |&gt; \n  conf_mat(truth, predicted_threshold) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"lightblue\") +\n  coord_equal()\n\n\n\n\n\n\nFigure 3: The confusion matrix with the adjusted threshold value.\n\n\n\n\nNotice how now the lower-left and upper-right quadrants have equal values. This means that the number of observations classified as Class 1 matches the amount in the training data.\nHowever, this can come at the cost of reduced accuracy. Funnily enough this is not the case for this particular dataset, but it was for the dataset I was using.\n\nlibrary(tidyr)\n\ntwo_class_adjusted |&gt; \n  select(truth, predicted, predicted_threshold) |&gt; \n  pivot_longer(\n    cols            = c(predicted, predicted_threshold),\n    names_to        = \"threshold\",\n    values_to       = \"predicted\",\n    names_transform = \\(x) ifelse(x == \"predicted\", 0.5, threshold)\n  ) |&gt; \n  group_by(threshold) |&gt; \n  accuracy(truth, predicted) |&gt; \n  select(threshold, accuracy = .estimate)\n#&gt; # A tibble: 2 × 2\n#&gt;   threshold accuracy\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0.5      0.838\n#&gt; 2     0.610    0.86\n\nOf course, in reality you would want to find the best threshold for each training set and average them to apply to the test set."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Reading Log",
    "section": "",
    "text": "I have been focusing on reading important philosophical and psychological texts as well as textbooks pertaining to R, machine learning and data science.\nI also listen to podcasts and lectures to center my mind on the important things in life. Centering around being better disciplined and having mental fortitude alongside physical strength and intelligence."
  },
  {
    "objectID": "books.html#section",
    "href": "books.html#section",
    "title": "Reading Log",
    "section": "2025",
    "text": "2025\nMy goal for this year is 12 books.\n\n\n\n\n\n\n\nHands-On Machine Learning with R (2020) by Bradley Boehmke and Brandon Greenwell.\nMere Christianity (1952) by C. S. Lewis."
  },
  {
    "objectID": "books.html#section-1",
    "href": "books.html#section-1",
    "title": "Reading Log",
    "section": "2024",
    "text": "2024\nI started reading more deliberately in the fall.\n\nThe Bible - Most of the Old Testament I read in the NIV, but the rest I read in the ESV.\nAwesome Encounters with God: The Miracles of Jesus (2003) by Dr. Russel Bone.\nggplot2: Elegant Graphics for Data Analysis (3rd edition) by Wickham, Navarro and Pedersen.\nHillbilly Elegy (2016) by Vice President J. D. Vance.\nR for Data Science (2nd edition, 2017) by Hadley Wickham.\nThe American Story: The Beginnings (2020) by David and Tim Barton.\nThe Hobbit (1995 edition) by J. R. R. Tolkien."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nMA in Economics | August 2023 - May 2024\nCertificate of Data Analytics in Economics | August 2023 - May 2024\nBS in Data Science | August 2020 - May 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "S. L. Carter",
    "section": "",
    "text": "University of Toledo | Toledo, Ohio\nMA in Economics | August 2023 - May 2024\nCertificate of Data Analytics in Economics | August 2023 - May 2024\nBS in Data Science | August 2020 - May 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "S. L. Carter",
    "section": "Experience",
    "text": "Experience\n\nUniversity of Toledo | Toledo, Ohio\n\n\n\n\n\n\nBusiness Insights Analyst | May 2024 - Present\n\n\n\n\n\n\nProjects\n\nAnalyzed enrollment and admissions data to develop a detailed simulation and forecast of university enrollment.\nBuilt several Shiny applications for streamlining data analysis and visualization to better inform decision-makers in a dynamic and intuitive manner.\nDesigned a number of webscraping and data aquisition tools to ensure straightforward data sharing and analysis.\nProduced a streamlined process to download and transform public educational data into a language agnostic format.\nDeveloped a check to search for unusual work history and flag for review.\n\n\n\nSpecialties\n\nR\n\nTidyverse, tidymodels, and fpp3 primarily, in addition to others.\n\nShiny for R\nQuarto and RMarkdown\nSQL\n\n\n\n\n\n\n\n\n\n\n\nGraduate Assistant | Aug 2023 - May 2024\n\n\n\n\n\nSpecialties\n\nR\nShiny for R\nRMarkdown\n\n\n\n\n\n\n\n\n\n\nUndergraduate Research Assistant | Oct 2022 - Aug 2023\n\n\n\n\n\n\nProjects\n\nDeveloped software for performing high quality electrical measurements with efficiency and precision. Improved experimental speed and consistency through software improvements and advances to the physical experimentation system.\nDeveloped workflows to improve quality and consistency for analyzing experimental data output.\nPerformed electrical and optical photovoltaic measurements.\nSee below for papers in which the software I largely developed was utilized, even beyond my time of employment.\n\n\n\nSpecialties\n\nPython\n\nNumPy, scikit-learn, and pandas."
  },
  {
    "objectID": "index.html#sec-publications",
    "href": "index.html#sec-publications",
    "title": "S. L. Carter",
    "section": "Publications",
    "text": "Publications\nFor the 2024 and 2025 articles, I was a major contributer to the software used to perform the transient photovoltage/photocurrent experiments as well as developing scripts to automate initial analysis. For the 2023 article, contributed a Python script to automate a basic analysis of some data; a relatively small part of the project.\n\n2025\n\nAbasi Abudulimu, Sheng Fu, Nadeesha Katakumbura, Nannan Sun, Steven Carter, Tyler Brau, Lei Chen, Manoj Rajakaruna, Jared Friedl, Zhaoning Song, Adam B. Phillips, Michael J. Heben, Yanfa Yan and Randy J. Ellingson. 2025. “Enchancing Understanding of Recombination Mechanisms in High-Performance Tin-Lead Perovskite Solar Cells.” Cell Reports Physical Science 6 (1). https://doi.org/10.1016/j.xcrp.2024.102349.\n\n\n\n2024\n\nAbudulimu, Abasi, Adam Phillips, Steven Carter, Sabin Neupane, Deng-Bing Li, Tyler Brau, Jared Friedl, Yanfa Yan, Michael Heben, and Randy Ellingson. 2024. “Transient Photovoltage as a Tool for Probing Carrier Dynamics in CdTe-Based Solar Cells.” In 2024 IEEE 52nd Photovoltaic Specialist Conference (PVSC), 1220–20. IEEE. https://doi.org/10.1109/pvsc57443.2024.10749335.\nAbudulimu, Abasi, Steven Carter, Adam B. Phillips, Deng‐Bing Li, Sabin Neupane, Tyler Brau, Jared Friedl, et al. 2024. “Comprehensive Study of Carrier Recombination in High‐efficiency CdTe Solar Cells Using Transient Photovoltage.” Solar RRL 8 (10). https://doi.org/10.1002/solr.202400131.\n\n\n\n2023\n\nPhillips, Adam B., Jared D. Friedl, Philip Ottinger, Steven L. Carter, Zhaoning Song, Abasi Abudulimu, Ebin Bastola, et al. 2023. “Understanding and Advancing Bifacial Thin Film Solar Cells Under Dual Illumination.” Solar RRL 7 (21). https://doi.org/10.1002/solr.202300545."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Balancing Classification Errors for Aggregate Consistency\n\n\n\n\n\n\nClassification\n\n\nR\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\nNov 29, 2024\n\n\nSteven Carter\n\n\n\n\n\n\nNo matching items"
  }
]